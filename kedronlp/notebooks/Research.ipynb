{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding queries and fetching the most similar results\n",
    "\n",
    "Questions to a model an be asked using embedding based search. Embeddings are simple to implement and work especially well with questions, as questions often don't lexically overlap with their answers. Embeddings measure the relatedness of text strings and are commonly used for:\n",
    "- Search (where results are ranked by relevance to a query string)\n",
    "- Clustering (where text strings are grouped by similarity)\n",
    "- Recommendations (where items with related text strings are recommended)\n",
    "- Diversity measurement (where similarity distributions are analyzed)\n",
    "- Classification (where text strings are classified by their most similar label)\n",
    "\n",
    "An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding a user query is highly dependent on the model being employed for encoding the query. These pre trained large language models will often provide ways to embed or encode queries to the same vector space as the model was trained on, so that the relatedness of the query to the documents can be measured effectively. Commonly used models include:\n",
    "- BERT by Google\n",
    "- Llama 2 by Meta AI\n",
    "- DistilBERT by HuggingFace\n",
    "- GPT-3 by OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of using the OpenAI API to embed user query for searching relevant answers in the documents dataset. It should be noted that closed models have pricing on API calls. We can nonetheless use models like BERT and DistilBERT for free, as they are open source. The following code is for illustrative purposes only. To retrieve the most relevant documents we use the cosine similarity between the embedding vectors of the query and each document, and return the highest scored documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from openai.embeddings_utils import cosine_similarity, get_embedding\n",
    "\n",
    "client = OpenAI()\n",
    "df = pd.read_csv(datafile_path)\n",
    "\n",
    "\n",
    "# As seen here, the API provided by the model developers can be used to create embeddings\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "\n",
    "# Use the cosine similarity to find the top n most similar embeddings\n",
    "def search(df, description, n=3):\n",
    "    embedding = get_embedding(description, model=\"text-embedding-ada-002\")\n",
    "    df[\"similarities\"] = df.ada_embedding.apply(lambda x: cosine_similarity(x, embedding))\n",
    "    res = df.sort_values(\"similarities\", ascending=False).head(n)\n",
    "    return res\n",
    "\n",
    "\n",
    "# embed query and search for top 3 results in the dataframe\n",
    "res = search(df, \"query\", n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also have to tokenize our query first before creating embeddings. This step is crucial for models that use subword tokenization like BERT. After that, we can apply the language model to generate embeddings for the individual tokens. For BERT or similar models, one will typically use the embeddings of the [CLS] token, which is usually used as a sentence-level embedding. Token embeddings are also aggregated into a single vector using common methods like mean pooling, max pooling, or using the [CLS] token. For ensuring that the embeddings are on a comparable scale, we can also normalize them, which is very crucial if we are using cosine similarity to measure the relatedness between the query and the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is another illustrative example using BERT and the transformers library by Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize and encode the reference text using the same tokenizer used for the language model.\n",
    "reference_text = \"reference text goes here.\"\n",
    "\n",
    "encoded_reference = tokenizer.encode_plus(\n",
    "    reference_text, add_special_tokens=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "\n",
    "# example function to answer questions\n",
    "def answer_question(question):\n",
    "    encoded_question = tokenizer.encode_plus(\n",
    "        question, add_special_tokens=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_reference[\"input_ids\"]\n",
    "    attention_mask = encoded_reference[\"attention_mask\"]\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        start_positions=None,\n",
    "        end_positions=None,\n",
    "    )\n",
    "\n",
    "    start_idx = torch.argmax(outputs.start_logits)\n",
    "    end_idx = torch.argmax(outputs.end_logits)\n",
    "\n",
    "    # convert the tokens back to strings\n",
    "    answer = tokenizer.convert_tokens_to_string(\n",
    "        tokenizer.convert_ids_to_tokens(input_ids[0][start_idx : end_idx + 1])\n",
    "    )\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a basic example of usage of such a model for embedding queries and retrieving the most similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How tall is the Golden Gate Bridge?\"\n",
    "\n",
    "response = answer_question(question=question)\n",
    "print(\"Answer:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize,\n",
    "-  Tokenization: Break down the user-question into individual tokens using a tokenizer provided by the LLM.\n",
    " - Word Embeddings: Represent each token as a vector of numerical values. This is typically done using a pre-trained word embedding matrix, which maps words to their corresponding vectors.\n",
    "- Contextual Embeddings: Apply the LLM's contextualized embedding layer to the token vectors. This layer takes into account the surrounding context of each word, providing more nuanced representations.\n",
    "-  Question Embedding: Aggregate the contextualized word embeddings to form a single vector representation for the entire question.\n",
    " - Normalization: Normalize the question embedding vector to ensure it has a unit length. Helps standardize the comparison of embeddings and improve the accuracy of similarity measures like cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticSearch\n",
    "\n",
    "If one is using ElasticSearch, we can use the \"More Like This\" (MLT) query or the \"Similarity Search\" feature. ElasticSearch also provides a k-nearest neighbor (kNN) search feature to a given query vector. This feature is useful for tasks such as semantic search\n",
    "\n",
    "To use the kNN search feature, we first need to index your documents with their corresponding vector representations. This can be done using text embedding models, such as Word2Vec, GloVe, or ELMo. Once the documents are indexed, we can use the match_all query and the knn_field parameter to find the top-k most similar documents to a given query vector. For example, the following query will find the top-5 most similar documents to the query vector [0.321, 0.432, 0.287, 0.123] in the index my_index with the field vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GET / my_index / _search\n",
    "{\n",
    "    \"query\": {\"match_all\": {}},\n",
    "    \"knn_field\": {\n",
    "        \"field\": \"vector\",\n",
    "        \"query\": [0.321, 0.432, 0.287, 0.123],\n",
    "        \"size\": 5,  # number of nearest neighbors to return\n",
    "    },\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
